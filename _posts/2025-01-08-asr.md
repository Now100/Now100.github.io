---
layout: post
title: 'ASR(Automatic Speech Recognition) 개념 정리'
date: 2025-01-08 11:19 +0900
categories: ['음성인식']
tags: ['ASR', '음성인식']
published: true
sitemap: true
math: true
---
자동 음성 인식(Automatic Speech Recognition, ASR)이란 인간의 음성 신호를 디지털 신호로 분석하고, 이를 텍스트 데이터로 변환하는 기술입니다. Speech-to-Text(STT)라는 이름으로도 불립니다. 
이 포스트에서는 해당 기술에 대한 고전적 접근법부터 단계적으로 살펴본 다음, 시간의 변화에 따라 어떻게 발전해왔는지에 대해 기술해보겠습니다.

# 1. 음성 신호 수집 & 전처리
당연한 이야기지만, 음성 신호를 텍스트로 옮기기 위해서는 가장 먼저 음성신호를 입력받아야 합니다. 이때, 연속적인 아날로그 신호인 인간의 음성을 불연속적 디지털 신호로 옮기기 위해서는 샘플링 과정이 필요합니다.
샘플링이란 특정 신호를 일정 주기로 기록하는 것을 말합니다. 이때 기록을 보다 정확히 하기 위해선 짧은 주기로 기록하는 것이 좋을 것입니다. 하지만 너무 짧은 주기로 기록하게 되면 불필요한 정보들이 많아질 것입니다. 그렇다면 어느 정도의 주기가 아날로그 신호의 정보를 보존하기 위해 적당할까요?
**나이퀴스트 이론(Nyquist Theorem)** 에 따르면, 신호의 최대 주파수 성분(
$f_{max}$)의 두 배 이상의 샘플링 주파수($f_s$)로 샘플링하면, 원래의 아날로그 신호를 손실 없이 복원할 수 있습니다.

$$
f_{max} \geq 2 f_s
$$

수식때문에 머리가 아플수도 있겠으나, 어찌보면 당연한 이야깁니다. 어떤 신호의 주파수가 주어져있다면, 그 신호를 손실없이 복원하기 위해서는 그 주파수의 두배의 주파수로만 샘플링을 진행해도 충분하다는 것이죠.

<div align="center">
  <img src="../assets/img/2025-01-08-asr/image.png" alt="샘플링 주파수">
  <p><em>샘플링 주파수에 따른 샘플링 차이</em></p>
</div>

인간의 가청 음역대는 20~20,000Hz 이지만, 음성 인식만이 목적이라면 그렇게 높은 주파수 대역까지 신경쓸 필요는 없습니다. 사람의 음성이 가질 수 있는 주파수 대역은 100~10,000Hz 정도이며, 일상 회화는 200~6,000Hz 영역으로 제한되기 때문입니다.
따라서 음성 인식에서 주로 사용되는 샘플링 주파수는 16kHz입니다. 해당 샘플링 주파수로 음성 신호의 정보를 효율적으로 복원해 낼 수 있습니다.

<div align="center">
  <img src="../assets/img/2025-01-08-asr/image01.png" alt="파형">
</div>

샘플링을 거쳐 음성 데이터의 파형을 얻어냈으나 이 원시 음성 데이터를 그대로 사용하지는 않습니다.
원시 음성 데이터만으로는 여러 한계를 갖는데, 화자에 따라 같은 단어를 발음한 데이터라도 다른 신호로 표현될 수 있으며, 같은 화자가 발화한 같은 단어일지라도 같은 신호를 얻을 수 있다는 보장이 없습니다. 말하는 속도, 억양, 음량에 따라서 신호는 가변적으로 나타날 것입니다. 이와 같은 이유로, 일반적으로는 이 원시 음성 데이터에서 특징을 추출합니다. 
가장 간단하면서도 강력한 특징 추출은 스펙트로그램으로 얻어낼 수 있습니다.

**스펙트로그램(Spectrogram)** 은 소리나 신호의 주파수 성분이 시간에 따라 어떻게 변화하는지 시각적으로 표현한 그래프입니다. 이는 음성, 음악, 환경 소음 등 다양한 신호를 분석하는 데 사용되며, 시간(Time), 주파수(Frequency), 그리고 진폭(Amplitude) 이라는 세 가지 차원을 포함합니다.

**스펙트로그램의 구성**
- **X축** (시간): 신호가 시간에 따라 어떻게 변화하는지를 나타냅니다.
- **Y축** (주파수): 신호에 포함된 주파수 성분을 나타냅니다.
- **색상 또는 밝기** (진폭/에너지): 특정 시간과 주파수에서의 신호 강도를 나타냅니다. 일반적으로 더 밝거나 짙은 색은 더 높은 진폭(소리의 크기)을 의미합니다.
<div align="center">
  <img src="../assets/img/2025-01-08-asr/image02.png" alt="spectrogram">
  <p><em>영어 모음에 따른 스펙트로그램의 차이</em></p>
</div>

음성 언어의 음운은 해당 음운이 발화된 세기, 억양 등과 관계 없이 고유의 주파수 패턴을 가집니다. 이를 통해 우리는 음운 정보를 파악할 수 있습니다.
그렇다면 1차원 원시 음성 데이터로부터 이런 스펙트로그램은 어떻게 얻어내는 것일까요?
스펙트로그램은 **푸리에 변환**, 더 자세히 말하자면 **단시간 푸리에 변환(STFT, Short-Time Fourier Transform)** 을 통해 얻어낼 수 있습니다.
<div align="center">
  <img src="../assets/img/2025-01-08-asr/image03.png" alt="fourier">
</div>
푸리에 변환에 대해 간단히 소개하자면, 복잡한 신호를 간단한 신호들의 합으로 풀어내는 것을 말합니다. 이를 통해 우리는 1차원 음성 신호를 구성하는 주파수들을 파악할 수 있고, 이로부터 음운의 정보를 얻어낼 수 있습니다.

<div align="center">
  <img src="../assets/img/2025-01-08-asr/image04.png" alt="stft">
</div>
하지만 우리는 음성 신호 전체가 아닌 음운 하나하나의 정보를 파악해야하므로, 1차원 음성 신호를 잘게 쪼개 각 부분에서 음운에 대한 정보를 알아내야합니다. 이와 같이 신호를 일정한 시간 단위로 쪼개 각 신호에 대해 푸리에 변환을 적용하는 것을 단시간 푸리에 변환(STFT, Short-Time Fourier Transform)이라고 합니다.

윈도잉을 진행할때, 보통 윈도우 사이즈는 20~25ms 정도로 잡는데, 20~25ms의 짧은 시간 구간 안에서는 음운의 정보가 유지되기 때문입니다. 또한 신호의 연속성 보장을 위해 보통 10ms 정도의 겹치는 구간을 두고 윈도잉을 진행합니다.

<div align="center">
  <img src="../assets/img/2025-01-08-asr/image05.png" alt="달팽이관">
</div>
하지만 여기서 끝이 아니다?! 여기서 잠깐 인간의 청각 능력에 대해 이야기해보겠습니다. 인간의 청각 능력은 소리의 주파수 차이를 선형적으로 인식하지 않습니다. 이는 달팽이관의 구조에서 기인한 것으로, 달팽이관은 저주파수 대역의 소리에 대해서 민감하게 반응하는 반면, 고주파수 대역의 소리에 대해서는 덜 민감하게 반응합니다.

<div align="center">
  <img src="../assets/img/2025-01-08-asr/image06.png" alt="달팽이관">
</div>

이러한 청각 특징을 반여하기 위해 우리는 Mel filterbank라는 필터를 통해 저주파수 대역의 정보를 강화시킨 **Mel spectrogram**을 얻게됩니다.
여기서 로그 변환과 Discrete Cosine Transform (DCT) 연산을 통해 특징 벡터를 얻을 수 있는데, 이를 **MFCC (Mel-Frequency Cepstral Coefficient)** 라고 부릅니다. 
이렇게 음성 데이터를 벡터화 하여 사용할 수 있는데, 딥러닝 시대에서는 mel spectrogram 추출까지만 전처리로 진행하고 그 이후는 스펙트로그램 이미지만을 입력으로 사용하는 것이 일반적이라고합니다.

작성중...